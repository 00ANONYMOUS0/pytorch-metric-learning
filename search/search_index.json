{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Metric Learning Installation pip install pytorch_metric_learning Overview","title":"Home"},{"location":"#pytorch-metric-learning","text":"","title":"PyTorch Metric Learning"},{"location":"#installation","text":"pip install pytorch_metric_learning","title":"Installation"},{"location":"#overview","text":"","title":"Overview"},{"location":"losses/","text":"Losses All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) Or if you are using a loss in conjunction with a miner: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output ) AngularLoss Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha , triplets_per_anchor = 100 , ** kwargs ) Parameters : alpha : The angle (as described in the paper), specified in degrees. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. ArcFaceLoss ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The angular margin penalty in degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) BaseMetricLossFunction All loss functions extend this class. losses . BaseMetricLossFunction ( normalize_embeddings = True , num_class_per_param = None , learnable_param_names = None ) Parameters : normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before the loss is computed. num_class_per_param : If learnable_param_names is set, then this represents the number of classes for each parameter. If your parameters don't have a separate value for each class, then you can leave this at None. learnable_param_names : A list of strings where each element is the name of attributes that should be converted to nn.Parameter. If None, then no parameters are converted. Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError ContrastiveLoss losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , use_similarity = False , power = 1 , avg_non_zero_only = True , ** kwargs ): Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. use_similarity : If True, will use dot product between vectors instead of euclidean distance. power : Each pair's loss will be raised to this power. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss. FastAPLoss Deep Metric Learning to Rank losses . FastAPLoss ( num_bins , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision GenericPairLoss losses . GenericPairLoss ( use_similarity , iterate_through_loss , squared_distances = False , ** kwargs ) Parameters : use_similarity : Set to True if the loss function uses pairwise similarity (dot product of each embedding pair). Otherwise, euclidean distance will be used. iterate_through_loss : If True, then pairs are passed iteratively to self.pair_based_loss, by going through each sample in a batch, and selecting just the positive and negative pairs containing that sample. Otherwise, the pairs are passed to self.pair_based_loss all at once. squared_distances : If True, then the euclidean distance will be squared. Required Implementations : def pair_based_loss ( self , pos_pairs , neg_pairs , pos_pair_anchor_labels , neg_pair_anchor_labels ): raise NotImplementedError GeneralizedLiftedStructureLoss MarginLoss Sampling Matters in Deep Embedding Learning MultiSimilarityLoss Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning NCALoss Neighbourhood Components Analysis](https://www.cs.toronto.edu/~hinton/absps/nca.pdf) NormalizedSoftmaxLoss Classification is a Strong Baseline for DeepMetric Learning NPairsLoss Improved Deep Metric Learning with Multi-class N-pair Loss Objective ProxyNCALoss No Fuss Distance Metric Learning using Proxies SignalToNoiseRatioContrastiveLoss Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning SoftTripleLoss SoftTriple Loss: Deep Metric Learning Without Triplet Sampling TripletMarginLoss","title":"Losses"},{"location":"losses/#losses","text":"All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) Or if you are using a loss in conjunction with a miner: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output )","title":"Losses"},{"location":"losses/#angularloss","text":"Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha , triplets_per_anchor = 100 , ** kwargs ) Parameters : alpha : The angle (as described in the paper), specified in degrees. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used.","title":"AngularLoss"},{"location":"losses/#arcfaceloss","text":"ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The angular margin penalty in degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)","title":"ArcFaceLoss"},{"location":"losses/#basemetriclossfunction","text":"All loss functions extend this class. losses . BaseMetricLossFunction ( normalize_embeddings = True , num_class_per_param = None , learnable_param_names = None ) Parameters : normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before the loss is computed. num_class_per_param : If learnable_param_names is set, then this represents the number of classes for each parameter. If your parameters don't have a separate value for each class, then you can leave this at None. learnable_param_names : A list of strings where each element is the name of attributes that should be converted to nn.Parameter. If None, then no parameters are converted. Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError","title":"BaseMetricLossFunction"},{"location":"losses/#contrastiveloss","text":"losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , use_similarity = False , power = 1 , avg_non_zero_only = True , ** kwargs ): Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. use_similarity : If True, will use dot product between vectors instead of euclidean distance. power : Each pair's loss will be raised to this power. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss.","title":"ContrastiveLoss"},{"location":"losses/#fastaploss","text":"Deep Metric Learning to Rank losses . FastAPLoss ( num_bins , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision","title":"FastAPLoss"},{"location":"losses/#genericpairloss","text":"losses . GenericPairLoss ( use_similarity , iterate_through_loss , squared_distances = False , ** kwargs ) Parameters : use_similarity : Set to True if the loss function uses pairwise similarity (dot product of each embedding pair). Otherwise, euclidean distance will be used. iterate_through_loss : If True, then pairs are passed iteratively to self.pair_based_loss, by going through each sample in a batch, and selecting just the positive and negative pairs containing that sample. Otherwise, the pairs are passed to self.pair_based_loss all at once. squared_distances : If True, then the euclidean distance will be squared. Required Implementations : def pair_based_loss ( self , pos_pairs , neg_pairs , pos_pair_anchor_labels , neg_pair_anchor_labels ): raise NotImplementedError","title":"GenericPairLoss"},{"location":"losses/#generalizedliftedstructureloss","text":"","title":"GeneralizedLiftedStructureLoss"},{"location":"losses/#marginloss","text":"Sampling Matters in Deep Embedding Learning","title":"MarginLoss"},{"location":"losses/#multisimilarityloss","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning","title":"MultiSimilarityLoss"},{"location":"losses/#ncaloss","text":"Neighbourhood Components Analysis](https://www.cs.toronto.edu/~hinton/absps/nca.pdf)","title":"NCALoss"},{"location":"losses/#normalizedsoftmaxloss","text":"Classification is a Strong Baseline for DeepMetric Learning","title":"NormalizedSoftmaxLoss"},{"location":"losses/#npairsloss","text":"Improved Deep Metric Learning with Multi-class N-pair Loss Objective","title":"NPairsLoss"},{"location":"losses/#proxyncaloss","text":"No Fuss Distance Metric Learning using Proxies","title":"ProxyNCALoss"},{"location":"losses/#signaltonoiseratiocontrastiveloss","text":"Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning","title":"SignalToNoiseRatioContrastiveLoss"},{"location":"losses/#softtripleloss","text":"SoftTriple Loss: Deep Metric Learning Without Triplet Sampling","title":"SoftTripleLoss"},{"location":"losses/#tripletmarginloss","text":"","title":"TripletMarginLoss"}]}