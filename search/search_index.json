{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Metric Learning \u00b6 Installation: \u00b6 Conda: conda install pytorch_metric_learning -c metric-learning Pip: pip install pytorch_metric_learning Overview \u00b6 Let\u2019s try the vanilla triplet margin loss . In all examples, embeddings is assumed to be of size (N, embedding_size), and labels is of size (N). from pytorch_metric_learning import losses loss_func = losses . TripletMarginLoss ( margin = 0.1 ) loss = loss_func ( embeddings , labels ) Loss functions typically come with a variety of parameters. For example, with the TripletMarginLoss, you can control how many triplets per sample to use in each batch. You can also use all possible triplets within each batch: loss_func = losses . TripletMarginLoss ( triplets_per_anchor = \"all\" ) Sometimes it can help to add a mining function: from pytorch_metric_learning import miners , losses miner = miners . MultiSimilarityMiner ( epsilon = 0.1 ) loss_func = losses . TripletMarginLoss ( margin = 0.1 ) hard_pairs = miner ( embeddings , labels ) loss = loss_func ( embeddings , labels , hard_pairs ) In the above code, the miner finds positive and negative pairs that it thinks are particularly difficult. Note that even though the TripletMarginLoss operates on triplets, it\u2019s still possible to pass in pairs. This is because the library automatically converts pairs to triplets and triplets to pairs, when necessary. In general, all loss functions take in embeddings and labels, with an optional indices_tuple argument (i.e. the output of a miner): # From BaseMetricLossFunction def forward ( self , embeddings , labels , indices_tuple = None ) And all mining functions take in embeddings and labels: # From BaseMiner def forward ( self , embeddings , labels )","title":"Home"},{"location":"#pytorch-metric-learning","text":"","title":"PyTorch Metric Learning"},{"location":"#installation","text":"Conda: conda install pytorch_metric_learning -c metric-learning Pip: pip install pytorch_metric_learning","title":"Installation:"},{"location":"#overview","text":"Let\u2019s try the vanilla triplet margin loss . In all examples, embeddings is assumed to be of size (N, embedding_size), and labels is of size (N). from pytorch_metric_learning import losses loss_func = losses . TripletMarginLoss ( margin = 0.1 ) loss = loss_func ( embeddings , labels ) Loss functions typically come with a variety of parameters. For example, with the TripletMarginLoss, you can control how many triplets per sample to use in each batch. You can also use all possible triplets within each batch: loss_func = losses . TripletMarginLoss ( triplets_per_anchor = \"all\" ) Sometimes it can help to add a mining function: from pytorch_metric_learning import miners , losses miner = miners . MultiSimilarityMiner ( epsilon = 0.1 ) loss_func = losses . TripletMarginLoss ( margin = 0.1 ) hard_pairs = miner ( embeddings , labels ) loss = loss_func ( embeddings , labels , hard_pairs ) In the above code, the miner finds positive and negative pairs that it thinks are particularly difficult. Note that even though the TripletMarginLoss operates on triplets, it\u2019s still possible to pass in pairs. This is because the library automatically converts pairs to triplets and triplets to pairs, when necessary. In general, all loss functions take in embeddings and labels, with an optional indices_tuple argument (i.e. the output of a miner): # From BaseMetricLossFunction def forward ( self , embeddings , labels , indices_tuple = None ) And all mining functions take in embeddings and labels: # From BaseMiner def forward ( self , embeddings , labels )","title":"Overview"},{"location":"losses/","text":"Losses \u00b6 All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) Or if you are using a loss in conjunction with a miner: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output ) AngularLoss \u00b6 Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha , triplets_per_anchor = 100 , ** kwargs ) Parameters : alpha : The angle (as described in the paper), specified in degrees. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. ArcFaceLoss \u00b6 ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The angular margin penalty in degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) BaseMetricLossFunction \u00b6 All loss functions extend this class and therefore inherit its __init__ parameters. losses . BaseMetricLossFunction ( normalize_embeddings = True , num_class_per_param = None , learnable_param_names = None ) Parameters : normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before the loss is computed. num_class_per_param : If learnable_param_names is set, then this represents the number of classes for each parameter. If your parameters don't have a separate value for each class, then you can leave this at None. learnable_param_names : A list of strings where each element is the name of attributes that should be converted to nn.Parameter. If None, then no parameters are converted. Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError ContrastiveLoss \u00b6 losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , use_similarity = False , power = 1 , avg_non_zero_only = True , ** kwargs ): Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. use_similarity : If True, will use dot product between vectors instead of euclidean distance. power : Each pair's loss will be raised to this power. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss. FastAPLoss \u00b6 Deep Metric Learning to Rank losses . FastAPLoss ( num_bins , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision GenericPairLoss \u00b6 losses . GenericPairLoss ( use_similarity , iterate_through_loss , squared_distances = False , ** kwargs ) Parameters : use_similarity : Set to True if the loss function uses pairwise similarity (dot product of each embedding pair). Otherwise, euclidean distance will be used. iterate_through_loss : If True, then pairs are passed iteratively to self.pair_based_loss, by going through each sample in a batch, and selecting just the positive and negative pairs containing that sample. Otherwise, the pairs are passed to self.pair_based_loss all at once. squared_distances : If True, then the euclidean distance will be squared. Required Implementations : def pair_based_loss ( self , pos_pairs , neg_pairs , pos_pair_anchor_labels , neg_pair_anchor_labels ): raise NotImplementedError GeneralizedLiftedStructureLoss \u00b6 losses . GeneralizedLiftedStructureLoss ( neg_margin , ** kwargs ) Parameters : neg_margin : The margin in the expression e^(margin - negative_distance) MarginLoss \u00b6 Sampling Matters in Deep Embedding Learning losses . MarginLoss ( margin , nu , beta , triplets_per_anchor = 100 , ** kwargs ) Parameters : margin : The radius of the minimalbuffer between positive and negative pairs. nu : The regularization weight for the magnitude of beta. beta : The center of the minimal buffer between positive and negative pairs. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. To make beta a learnable parameter (as done in the paper), pass in the keyword argument: learnable_param_names = [ \"beta\" ] You can then pass the loss function's parameters() to any PyTorch optimizer. MultiSimilarityLoss \u00b6 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning losses . MultiSimilarityLoss ( alpha , beta , base = 0.5 , ** kwargs ) Parameters : alpha : The weight applied to positive pairs. beta : The weight applied to negative pairs. base : The offset applied to the exponent in the loss. NCALoss \u00b6 Neighbourhood Components Analysis losses . NCALoss ( softmax_scale = 1 , ** kwargs ) Parameters : softmax_scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) NormalizedSoftmaxLoss \u00b6 Classification is a Strong Baseline for Deep Metric Learning losses . NormalizedSoftmaxLoss ( temperature , embedding_size , num_classes , ** kwargs ) Parameters : temperature : The exponent divisor in the softmax funtion. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. NPairsLoss \u00b6 Improved Deep Metric Learning with Multi-class N-pair Loss Objective losses . NPairsLoss ( l2_reg_weight = 0 , ** kwargs ) Parameters : l2_reg_weight : The regularization weight for the L2 norm of the embeddings. ProxyNCALoss \u00b6 No Fuss Distance Metric Learning using Proxies losses . ProxyNCALoss ( num_classes , embedding_size , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. softmax_scale : See NCALoss SignalToNoiseRatioContrastiveLoss \u00b6 Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning losses . SignalToNoiseRatioContrastiveLoss ( pos_margin , neg_margin , regularizer_weight , avg_non_zero_only = True , ** kwargs ) Parameters : pos_margin : The noise-to-signal ratio over which positive pairs will contribute to the loss. neg_margin : The noise-to-signal ratio under which negative pairs will contribute to the loss. regularizer_weight : The regularizer encourages the embeddings to have zero-mean distributions. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss. SoftTripleLoss \u00b6 SoftTriple Loss: Deep Metric Learning Without Triplet Sampling losses . SoftTripleLoss ( embedding_size , num_classes , centers_per_class , la = 20 , gamma = 0.1 , reg_weight = 0.2 , margin = 0.01 , ** kwargs ) Parameters : embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. centers_per_class : The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) la : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) gamma : The similarity-to-centers multiplier. reg_weight : The regularization weight which encourages class centers to be close to each other. margin : The margin in the expression e^(similarities - margin). TripletMarginLoss \u00b6 lossses . TripletMarginLoss ( margin = 0.05 , distance_norm = 2 , power = 1 , swap = False , smooth_loss = False , avg_non_zero_only = True , triplets_per_anchor = 100 , ** kwargs ) Parameters : margin : The desired difference between the anchor-positive distance and the anchor-negative distance. distance_norm : The norm used when calculating distance between embeddings power : Each pair's loss will be raised to this power. swap : Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more. smooth_loss : Use the log-exp version of the triplet loss avg_non_zero_only : Only triplets that contribute non-zero loss will be used in the final loss. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used.","title":"Losses"},{"location":"losses/#losses","text":"All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) Or if you are using a loss in conjunction with a miner: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output )","title":"Losses"},{"location":"losses/#angularloss","text":"Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha , triplets_per_anchor = 100 , ** kwargs ) Parameters : alpha : The angle (as described in the paper), specified in degrees. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used.","title":"AngularLoss"},{"location":"losses/#arcfaceloss","text":"ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The angular margin penalty in degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)","title":"ArcFaceLoss"},{"location":"losses/#basemetriclossfunction","text":"All loss functions extend this class and therefore inherit its __init__ parameters. losses . BaseMetricLossFunction ( normalize_embeddings = True , num_class_per_param = None , learnable_param_names = None ) Parameters : normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before the loss is computed. num_class_per_param : If learnable_param_names is set, then this represents the number of classes for each parameter. If your parameters don't have a separate value for each class, then you can leave this at None. learnable_param_names : A list of strings where each element is the name of attributes that should be converted to nn.Parameter. If None, then no parameters are converted. Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError","title":"BaseMetricLossFunction"},{"location":"losses/#contrastiveloss","text":"losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , use_similarity = False , power = 1 , avg_non_zero_only = True , ** kwargs ): Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. use_similarity : If True, will use dot product between vectors instead of euclidean distance. power : Each pair's loss will be raised to this power. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss.","title":"ContrastiveLoss"},{"location":"losses/#fastaploss","text":"Deep Metric Learning to Rank losses . FastAPLoss ( num_bins , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision","title":"FastAPLoss"},{"location":"losses/#genericpairloss","text":"losses . GenericPairLoss ( use_similarity , iterate_through_loss , squared_distances = False , ** kwargs ) Parameters : use_similarity : Set to True if the loss function uses pairwise similarity (dot product of each embedding pair). Otherwise, euclidean distance will be used. iterate_through_loss : If True, then pairs are passed iteratively to self.pair_based_loss, by going through each sample in a batch, and selecting just the positive and negative pairs containing that sample. Otherwise, the pairs are passed to self.pair_based_loss all at once. squared_distances : If True, then the euclidean distance will be squared. Required Implementations : def pair_based_loss ( self , pos_pairs , neg_pairs , pos_pair_anchor_labels , neg_pair_anchor_labels ): raise NotImplementedError","title":"GenericPairLoss"},{"location":"losses/#generalizedliftedstructureloss","text":"losses . GeneralizedLiftedStructureLoss ( neg_margin , ** kwargs ) Parameters : neg_margin : The margin in the expression e^(margin - negative_distance)","title":"GeneralizedLiftedStructureLoss"},{"location":"losses/#marginloss","text":"Sampling Matters in Deep Embedding Learning losses . MarginLoss ( margin , nu , beta , triplets_per_anchor = 100 , ** kwargs ) Parameters : margin : The radius of the minimalbuffer between positive and negative pairs. nu : The regularization weight for the magnitude of beta. beta : The center of the minimal buffer between positive and negative pairs. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. To make beta a learnable parameter (as done in the paper), pass in the keyword argument: learnable_param_names = [ \"beta\" ] You can then pass the loss function's parameters() to any PyTorch optimizer.","title":"MarginLoss"},{"location":"losses/#multisimilarityloss","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning losses . MultiSimilarityLoss ( alpha , beta , base = 0.5 , ** kwargs ) Parameters : alpha : The weight applied to positive pairs. beta : The weight applied to negative pairs. base : The offset applied to the exponent in the loss.","title":"MultiSimilarityLoss"},{"location":"losses/#ncaloss","text":"Neighbourhood Components Analysis losses . NCALoss ( softmax_scale = 1 , ** kwargs ) Parameters : softmax_scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)","title":"NCALoss"},{"location":"losses/#normalizedsoftmaxloss","text":"Classification is a Strong Baseline for Deep Metric Learning losses . NormalizedSoftmaxLoss ( temperature , embedding_size , num_classes , ** kwargs ) Parameters : temperature : The exponent divisor in the softmax funtion. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset.","title":"NormalizedSoftmaxLoss"},{"location":"losses/#npairsloss","text":"Improved Deep Metric Learning with Multi-class N-pair Loss Objective losses . NPairsLoss ( l2_reg_weight = 0 , ** kwargs ) Parameters : l2_reg_weight : The regularization weight for the L2 norm of the embeddings.","title":"NPairsLoss"},{"location":"losses/#proxyncaloss","text":"No Fuss Distance Metric Learning using Proxies losses . ProxyNCALoss ( num_classes , embedding_size , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. softmax_scale : See NCALoss","title":"ProxyNCALoss"},{"location":"losses/#signaltonoiseratiocontrastiveloss","text":"Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning losses . SignalToNoiseRatioContrastiveLoss ( pos_margin , neg_margin , regularizer_weight , avg_non_zero_only = True , ** kwargs ) Parameters : pos_margin : The noise-to-signal ratio over which positive pairs will contribute to the loss. neg_margin : The noise-to-signal ratio under which negative pairs will contribute to the loss. regularizer_weight : The regularizer encourages the embeddings to have zero-mean distributions. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss.","title":"SignalToNoiseRatioContrastiveLoss"},{"location":"losses/#softtripleloss","text":"SoftTriple Loss: Deep Metric Learning Without Triplet Sampling losses . SoftTripleLoss ( embedding_size , num_classes , centers_per_class , la = 20 , gamma = 0.1 , reg_weight = 0.2 , margin = 0.01 , ** kwargs ) Parameters : embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. centers_per_class : The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) la : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) gamma : The similarity-to-centers multiplier. reg_weight : The regularization weight which encourages class centers to be close to each other. margin : The margin in the expression e^(similarities - margin).","title":"SoftTripleLoss"},{"location":"losses/#tripletmarginloss","text":"lossses . TripletMarginLoss ( margin = 0.05 , distance_norm = 2 , power = 1 , swap = False , smooth_loss = False , avg_non_zero_only = True , triplets_per_anchor = 100 , ** kwargs ) Parameters : margin : The desired difference between the anchor-positive distance and the anchor-negative distance. distance_norm : The norm used when calculating distance between embeddings power : Each pair's loss will be raised to this power. swap : Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more. smooth_loss : Use the log-exp version of the triplet loss avg_non_zero_only : Only triplets that contribute non-zero loss will be used in the final loss. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used.","title":"TripletMarginLoss"},{"location":"miners/","text":"Mining functions come in two flavors: pre-gradient miners output indices corresponding to a subset of the input batch. The idea is to use these miners with torch.no_grad(), and with a large input batch size. post-gradient miners output a tuple of indices: Pair miners output a tuple of size 4: (anchors, positives, anchors, negatives) Triplet miners output a tuple of size 3: (anchors, positives, negatives) Almost all miners are post-gradient miners. Post-gradient miners are used with loss functions as follows: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output ) AngularMiner \u00b6 miners . AngularMiner ( angle , ** kwargs ) Parameters angle : The miner will return triplets that form an angle greater than this input angle. The angle is computed as defined in the angular loss paper BaseMiner \u00b6 All miners extend this class and therefore inherit its __init__ parameters. miners . BaseMiner ( normalize_embeddings = True ) Parameters normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before any mining occurs. Required Implementations : def mine ( self , embeddings , labels ): raise NotImplementedError def output_assertion ( self , output ): raise NotImplementedError BatchHardMiner \u00b6 In Defense of the Triplet Loss for Person Re-Identification miners . BatchHardMiner ( use_similarity = False , squared_distances = False , ** kwargs ) Parameters use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared. DistanceWeightedMiner \u00b6 Sampling Matters in Deep Embedding Learning miners . DistanceWeightedMiner ( cutoff , nonzero_loss_cutoff , ** kwargs ) Parameters cutoff : Pairwise distances are clipped to this value if they fall below it. nonzero_loss_cutoff : Pairs that have distance greater than this are discarded. EmbeddingsAlreadyPackagedAsTriplets \u00b6 If your embeddings are already ordered sequentially as triplets, then use this miner to force your loss function to use the already-formed triplets. miners . EmbeddingsAlreadyPackagedAsTriplets () HDCMiner \u00b6 Hard-Aware Deeply Cascaded Embedding miners . HDCMiner ( filter_percentage , use_similarity = False , squared_distances = False , ** kwargs ) Parameters : filter_percentage : The percentage of pairs that will be returned. For example, if filter_percentage is 0.25, then the hardest 25% of pairs will be returned. The pool of pairs is either externally or internally set. See the important methods below for details. use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared. Important methods : # Pairs or triplets extracted from another miner, # and then passed in to HDCMiner using this function def set_idx_externally ( self , external_indices_tuple , labels ): self . a1 , self . p , self . a2 , self . n = lmu . convert_to_pairs ( external_indices_tuple , labels ) self . was_set_externally = True # Reset the internal state of the HDCMiner def reset_idx ( self ): self . a1 , self . p , self . a2 , self . n = None , None , None , None self . was_set_externally = False Example of passing another miner output to HDCMiner : minerA = miners . MultiSimilarityMiner ( epsilon = 0.1 ) minerB = miners . HDCMiner ( filter_percentage = 0.25 ) hard_pairs = minerA ( embeddings , labels ) minerB . set_idx_externally ( hard_pairs , labels ) very_hard_pairs = minerB ( embeddings , labels ) MaximumLossMiner \u00b6 This is a simple pre-gradient miner. It computes the loss for random subsets of the input batch, num_trials times. Then it returns the subset with the highest loss. miners . MaximumLossMiner ( loss_function , mining_function = None , num_trials = 5 , ** kwargs ) Parameters loss_function : The loss function used to compute the loss. mining_functions : Optional post-gradient mining function which extracts pairs/triplets for the loss function. num_trials : The number of random subsets to try. MultiSimilarityMiner \u00b6 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning miners . MultiSimilarityMiner ( epsilon , ** kwargs ) Parameters epsilon : Negative pairs are chosen if they have similarity greater than the hardest positive pair, minus this margin (epsilon). Positive pairs are chosen if they have similarity less than the hardest negative pair, plus this margin (epsilon). PairMarginMiner \u00b6 Returns positive and negative pairs that violate the specified margins. miners . PairMarginMiner ( pos_margin , neg_margin , use_similarity , squared_distances = False , ** kwargs ) Parameters pos_margin : The distance (or similarity) over (under) which positive pairs will be chosen. neg_margin : The distance (or similarity) under (over) which negative pairs will be chosen. use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared. TripletMarginMiner \u00b6 Returns hard, semihard, or all triplets. miners . TripletMarginMiner ( margin , type_of_triplets = \"all\" , ** kwargs ) Parameters margin : The difference between the anchor-positive distance and the anchor-negative distance. type_of_triplets : \"all\" means all triplets that violate the margin \"hard\" is a subset of \"all\", but the negative is closer to the anchor than the positive \"semihard\" is a subset of \"all\", but the negative is further from the anchor than the positive","title":"Miners"},{"location":"miners/#angularminer","text":"miners . AngularMiner ( angle , ** kwargs ) Parameters angle : The miner will return triplets that form an angle greater than this input angle. The angle is computed as defined in the angular loss paper","title":"AngularMiner"},{"location":"miners/#baseminer","text":"All miners extend this class and therefore inherit its __init__ parameters. miners . BaseMiner ( normalize_embeddings = True ) Parameters normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before any mining occurs. Required Implementations : def mine ( self , embeddings , labels ): raise NotImplementedError def output_assertion ( self , output ): raise NotImplementedError","title":"BaseMiner"},{"location":"miners/#batchhardminer","text":"In Defense of the Triplet Loss for Person Re-Identification miners . BatchHardMiner ( use_similarity = False , squared_distances = False , ** kwargs ) Parameters use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared.","title":"BatchHardMiner"},{"location":"miners/#distanceweightedminer","text":"Sampling Matters in Deep Embedding Learning miners . DistanceWeightedMiner ( cutoff , nonzero_loss_cutoff , ** kwargs ) Parameters cutoff : Pairwise distances are clipped to this value if they fall below it. nonzero_loss_cutoff : Pairs that have distance greater than this are discarded.","title":"DistanceWeightedMiner"},{"location":"miners/#embeddingsalreadypackagedastriplets","text":"If your embeddings are already ordered sequentially as triplets, then use this miner to force your loss function to use the already-formed triplets. miners . EmbeddingsAlreadyPackagedAsTriplets ()","title":"EmbeddingsAlreadyPackagedAsTriplets"},{"location":"miners/#hdcminer","text":"Hard-Aware Deeply Cascaded Embedding miners . HDCMiner ( filter_percentage , use_similarity = False , squared_distances = False , ** kwargs ) Parameters : filter_percentage : The percentage of pairs that will be returned. For example, if filter_percentage is 0.25, then the hardest 25% of pairs will be returned. The pool of pairs is either externally or internally set. See the important methods below for details. use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared. Important methods : # Pairs or triplets extracted from another miner, # and then passed in to HDCMiner using this function def set_idx_externally ( self , external_indices_tuple , labels ): self . a1 , self . p , self . a2 , self . n = lmu . convert_to_pairs ( external_indices_tuple , labels ) self . was_set_externally = True # Reset the internal state of the HDCMiner def reset_idx ( self ): self . a1 , self . p , self . a2 , self . n = None , None , None , None self . was_set_externally = False Example of passing another miner output to HDCMiner : minerA = miners . MultiSimilarityMiner ( epsilon = 0.1 ) minerB = miners . HDCMiner ( filter_percentage = 0.25 ) hard_pairs = minerA ( embeddings , labels ) minerB . set_idx_externally ( hard_pairs , labels ) very_hard_pairs = minerB ( embeddings , labels )","title":"HDCMiner"},{"location":"miners/#maximumlossminer","text":"This is a simple pre-gradient miner. It computes the loss for random subsets of the input batch, num_trials times. Then it returns the subset with the highest loss. miners . MaximumLossMiner ( loss_function , mining_function = None , num_trials = 5 , ** kwargs ) Parameters loss_function : The loss function used to compute the loss. mining_functions : Optional post-gradient mining function which extracts pairs/triplets for the loss function. num_trials : The number of random subsets to try.","title":"MaximumLossMiner"},{"location":"miners/#multisimilarityminer","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning miners . MultiSimilarityMiner ( epsilon , ** kwargs ) Parameters epsilon : Negative pairs are chosen if they have similarity greater than the hardest positive pair, minus this margin (epsilon). Positive pairs are chosen if they have similarity less than the hardest negative pair, plus this margin (epsilon).","title":"MultiSimilarityMiner"},{"location":"miners/#pairmarginminer","text":"Returns positive and negative pairs that violate the specified margins. miners . PairMarginMiner ( pos_margin , neg_margin , use_similarity , squared_distances = False , ** kwargs ) Parameters pos_margin : The distance (or similarity) over (under) which positive pairs will be chosen. neg_margin : The distance (or similarity) under (over) which negative pairs will be chosen. use_similarity : If True, will use dot product between vectors instead of euclidean distance. squared_distances : If True, then the euclidean distance will be squared.","title":"PairMarginMiner"},{"location":"miners/#tripletmarginminer","text":"Returns hard, semihard, or all triplets. miners . TripletMarginMiner ( margin , type_of_triplets = \"all\" , ** kwargs ) Parameters margin : The difference between the anchor-positive distance and the anchor-negative distance. type_of_triplets : \"all\" means all triplets that violate the margin \"hard\" is a subset of \"all\", but the negative is closer to the anchor than the positive \"semihard\" is a subset of \"all\", but the negative is further from the anchor than the positive","title":"TripletMarginMiner"},{"location":"samplers/","text":"Samplers \u00b6 Samplers are just extensions of the torch.utils.data.Sampler class, i.e. they are passed to a PyTorch Dataloader. The purpose of samplers is to determine how batches should be formed. This is also where any offline pair or triplet miners should exist. MPerClassSampler \u00b6 At every iteration, this will return m samples per class, assuming that the batch size is a multiple of m . For example, if your dataloader's batch size is 100, and m = 5, then 20 classes with 5 samples each will be returned. samplers . MPerClassSampler ( labels_to_indices , m , hierarchy_level = 0 ) Parameters : labels_to_indices : A list of dictionaries, where each dictionary maps labels to lists of indices that have that label. If your dataset has 1 label per element, then the list of dictionaries should contain just 1 dictionary. m : The number of samples per class to fetch at every iteration. If a class has less than m samples, then there will be duplicates in the returned batch. hierarchy_level : This is for multi-label datasets, and it indiates which level of labels will be used to form each batch. The default is 0, because most use-cases will have 1 label per datapoint. But for example, the iNaturalist dataset has 7 labels per datapoint, in which case hierarchy_level could be set to a number between 0 and 6. FixedSetOfTriplets \u00b6 When initialized, this class creates a fixed set of triplets. This is useful for determining the performance of algorithms in cases where the only ground truth data is a set of triplets. samplers . FixedSetOfTriplets ( labels_to_indices , num_triplets , hierarchy_level = 0 ) Parameters : labels_to_indices : A list of dictionaries, where each dictionary maps labels to lists of indices that have that label. If your dataset has N labels per datapoint, then the list of dictionaries should contain N dictionaries. num_triplets : The number of triplets to create. hierarchy_level : This is for multi-label datasets, and it indiates which level of labels will be used to form each batch. The default is 0, because most use-cases will have 1 label per datapoint. But for example, the iNaturalist dataset has 7 labels per datapoint, in which case hierarchy_level could be set to a number between 0 and 6.","title":"Samplers"},{"location":"samplers/#samplers","text":"Samplers are just extensions of the torch.utils.data.Sampler class, i.e. they are passed to a PyTorch Dataloader. The purpose of samplers is to determine how batches should be formed. This is also where any offline pair or triplet miners should exist.","title":"Samplers"},{"location":"samplers/#mperclasssampler","text":"At every iteration, this will return m samples per class, assuming that the batch size is a multiple of m . For example, if your dataloader's batch size is 100, and m = 5, then 20 classes with 5 samples each will be returned. samplers . MPerClassSampler ( labels_to_indices , m , hierarchy_level = 0 ) Parameters : labels_to_indices : A list of dictionaries, where each dictionary maps labels to lists of indices that have that label. If your dataset has 1 label per element, then the list of dictionaries should contain just 1 dictionary. m : The number of samples per class to fetch at every iteration. If a class has less than m samples, then there will be duplicates in the returned batch. hierarchy_level : This is for multi-label datasets, and it indiates which level of labels will be used to form each batch. The default is 0, because most use-cases will have 1 label per datapoint. But for example, the iNaturalist dataset has 7 labels per datapoint, in which case hierarchy_level could be set to a number between 0 and 6.","title":"MPerClassSampler"},{"location":"samplers/#fixedsetoftriplets","text":"When initialized, this class creates a fixed set of triplets. This is useful for determining the performance of algorithms in cases where the only ground truth data is a set of triplets. samplers . FixedSetOfTriplets ( labels_to_indices , num_triplets , hierarchy_level = 0 ) Parameters : labels_to_indices : A list of dictionaries, where each dictionary maps labels to lists of indices that have that label. If your dataset has N labels per datapoint, then the list of dictionaries should contain N dictionaries. num_triplets : The number of triplets to create. hierarchy_level : This is for multi-label datasets, and it indiates which level of labels will be used to form each batch. The default is 0, because most use-cases will have 1 label per datapoint. But for example, the iNaturalist dataset has 7 labels per datapoint, in which case hierarchy_level could be set to a number between 0 and 6.","title":"FixedSetOfTriplets"},{"location":"testers/","text":"Docs coming soon","title":"Testers"},{"location":"trainers/","text":"Trainers \u00b6 Trainers exist in this library because some metric learning algorithms are more than just loss or mining functions. Some algorithms require additional networks, data augmentations, learning rate schedules etc. The goal of the trainers module is to provide access to these type of metric learning algorithms. In general, trainers are used as follows: from pytorch_metric_learning import trainers t = trainers . SomeTrainingFunction ( ** kwargs ) t . train () BaseTrainer \u00b6 All trainers extend this class and therefore inherit its __init__ arguments. trainers . BaseTrainer ( models , optimizers , batch_size , loss_funcs , mining_funcs , iterations_per_epoch , dataset , data_device = None , loss_weights = None , label_mapper = None , sampler = None , collate_fn = None , record_keeper = None , lr_schedulers = None , gradient_clippers = None , freeze_trunk_batchnorm = False , label_hierarchy_level = 0 , dataloader_num_workers = 32 , data_and_label_getter = None ) Parameters : models : A dictionary of the form: {\"trunk\": trunk_model, \"embedder\": embedder_model} optimizers : A dictionary mapping strings to optimizers. The base class does not require any specific keys. For example, you could provide an empty dictionary, in which case no optimization will happen. Or you could provide just an optimizer for your trunk_model. But most likely, you'll want to pass in: {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}. batch_size : The number of elements that are retrieved at each iteration. loss_funcs : A dictionary mapping strings to loss functions. The required keys depend on the training method, but all methods are likely to require at least: {\"metric_loss\": loss_func}. mining_funcs : A dictionary mapping strings to mining functions. Pass in an empty dictionary, or one or more of the following keys: {\"pre_gradient_miner\": mining_func1, \"post_gradient_miner\": mining_func2} iterations_per_epoch : In this library, epochs are just a measure of the number of iterations that have passed. So _iterations_per_epoch is what actually defines what an \"epoch\" is. dataset : The dataset you want to train on. Note that training methods do not perform validation, so do not pass in your validation or test set. data_device : The device that you want to put batches of data on. If not specified, the trainer will put the data on any available GPUs. loss_weights : A dictionary mapping loss names to numbers. Each loss will be multiplied by the corresponding value in the dictionary. If not specified, then no loss weighting will occur. label_mapper : A function that takes in a label and returns another label. For example, it might be useful to move a set of labels ranging from 100-200 to a range of 0-100, in which case you could pass in lambda x: x-100 . If not specified, then the original labels are used. sampler : The sampler used by the dataloader. If not specified, then random sampling will be used. collate_fn : The collate function used by the dataloader. record_keeper : An optional record_keeper object. record_keeper is a useful package for logging data during training and testing. You can use trainers without record_keeper , but if you'd like to use it, then pip install record_keeper and visit the record_keeper repo to learn more. lr_scheduers : A dictionary of PyTorch learning rate schedulers. Each scheduler will be stepped at the end of every epoch. gradient_clippers : A dictionary of gradient clipping functions. Each function will be called before the optimizers. freeze_trunk_batchnorm : If True, then the BatchNorm parameters of the trunk model will be frozen during training. label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). dataloader_num_workers : The number of processes your dataloader will use to load data. data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels). MetricLossOnly \u00b6 This trainer just computes a metric loss from the output of your embedder network. trainers . MetricLossOnly ( ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"embedder\": embedder_model} loss_funcs : Must have the following form: {\"metric_loss\": loss_func} TrainWithClassifier \u00b6 This trainer is for the case where your architecture is trunk -> embedder -> classifier. It applies a metric loss to the output of the embedder network, and a classification loss to the output of the classifier network. trainers . TrainWithClassifier ( ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"embedder\": embedder_model, \"classifier\": classifier_model} loss_funcs : Must have the following form: {\"metric_loss\": loss_func1, \"classifier_loss\": loss_func2} CascadedEmbeddings \u00b6 This trainer is a generalization of Hard-Aware Deeply Cascaded Embedding . It splits the output of your embedder network, computing a separate loss for each section. In other words, the output of your embedder should be the concatenation of your cascaded models. trainers . CascadedEmbeddings ( embedding_sizes , ** kwargs ) Parameters : embedding_sizes: A list of integers, which represent the size of the output of each cascaded model. Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"embedder\": embedder_model} Optionally include key:values of the form \"classifier_%d\": classifier_model_%d. The integer appended to \"classifier_\" represents the cascaded model that the classifier will be appended to. For example, if the dictionary has classifier_0 and classifier_2, then the 0th and 2nd cascaded models will have classifier_model_0 and classifier_model_2 appended to them. loss_funcs : Must have the following form: {\"metric_loss_%d\": metric_loss_func_%d} Optionally include key:values of the form \"classifier_loss_%d\": classifier_loss_func_%d. The appended integer represents which cascaded model the loss applies to. mining_funcs : Must have the following form: {\"post_gradient_miner_%d\": mining_func_%d} *Optionally include \"pre_gradient_miner\": pre_gradient_miner DeepAdversarialMetricLearning \u00b6 This is an implementation of Deep Adversarial Metric Learning trainers . DeepAdversarialMetricLearning ( metric_alone_epochs = 0 , g_alone_epochs = 0 , g_triplets_per_anchor = 100 , ** kwargs ): Parameters : metric_alone_epochs : At the beginning of training, this many epochs will consist of only the metric_loss. g_alone_epochs : After metric_alone_epochs, this many epochs will consist of only the adversarial generator loss. g_triplets_per_anchor : The number of real triplets per sample that should be passed into the generator. For each real triplet, the generator will output a synthetic triplet. Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"embedder\": embedder_model} Optionally include \"classifier\": classifier_model loss_funcs : Must have the following form: {\"metric_loss\": metric_loss, \"g_adv_loss\": g_adv_loss, \"synth_loss\": synth_loss} Optionally include \"classifier_loss\": classifier_loss metric_loss applies to the embeddings of real data. g_adv_loss is the adversarial generator loss. Currently, only TripletMarginLoss and AngularLoss are supported synth_loss applies to the embeddings of the synthetic generator triplets. Currently, only TripletMarginLoss and AngularLoss are supported loss_weights : Must be one of the following: None {\"metric_loss\": weight1, \"g_adv_loss\": weight2, \"synth_loss\": weight3, \"g_reg_loss\": weight4, \"g_hard_lss\": weight5} Optionally include \"classifier_loss\": classifier_loss \"g_reg_loss\" and \"g_hard_loss\" refer to the regularization losses described in the paper. UnsupervisedEmbeddingsUsingAugmentations \u00b6 This is an implementation of a general approach that has been used in recent unsupervised learning papers, e.g. Unsupervised Embedding Learning via Invariant and Spreading Instance Feature and Unsupervised Deep Metric Learning via Auxiliary Rotation Loss . The idea is that augmented versions of a datapoint should be close to each other in the embedding space. trainers . UnsupervisedEmbeddingsUsingAugmentations ( transforms , data_and_label_setter = None , ** kwargs ) Parameters : transforms : A list of transforms. For every sample in a batch, each transform will be applied to the sample. If there are N transforms and the batch size is B, then there will be a total of B*N augmented samples. data_and_label_setter : A function that takes in a tuple of (augmented_data, pseudo_labels) and outputs whatever is expected by self.data_and_label_getter.","title":"Trainers"},{"location":"trainers/#trainers","text":"Trainers exist in this library because some metric learning algorithms are more than just loss or mining functions. Some algorithms require additional networks, data augmentations, learning rate schedules etc. The goal of the trainers module is to provide access to these type of metric learning algorithms. In general, trainers are used as follows: from pytorch_metric_learning import trainers t = trainers . SomeTrainingFunction ( ** kwargs ) t . train ()","title":"Trainers"},{"location":"trainers/#basetrainer","text":"All trainers extend this class and therefore inherit its __init__ arguments. trainers . BaseTrainer ( models , optimizers , batch_size , loss_funcs , mining_funcs , iterations_per_epoch , dataset , data_device = None , loss_weights = None , label_mapper = None , sampler = None , collate_fn = None , record_keeper = None , lr_schedulers = None , gradient_clippers = None , freeze_trunk_batchnorm = False , label_hierarchy_level = 0 , dataloader_num_workers = 32 , data_and_label_getter = None ) Parameters : models : A dictionary of the form: {\"trunk\": trunk_model, \"embedder\": embedder_model} optimizers : A dictionary mapping strings to optimizers. The base class does not require any specific keys. For example, you could provide an empty dictionary, in which case no optimization will happen. Or you could provide just an optimizer for your trunk_model. But most likely, you'll want to pass in: {\"trunk_optimizer\": trunk_optimizer, \"embedder_optimizer\": embedder_optimizer}. batch_size : The number of elements that are retrieved at each iteration. loss_funcs : A dictionary mapping strings to loss functions. The required keys depend on the training method, but all methods are likely to require at least: {\"metric_loss\": loss_func}. mining_funcs : A dictionary mapping strings to mining functions. Pass in an empty dictionary, or one or more of the following keys: {\"pre_gradient_miner\": mining_func1, \"post_gradient_miner\": mining_func2} iterations_per_epoch : In this library, epochs are just a measure of the number of iterations that have passed. So _iterations_per_epoch is what actually defines what an \"epoch\" is. dataset : The dataset you want to train on. Note that training methods do not perform validation, so do not pass in your validation or test set. data_device : The device that you want to put batches of data on. If not specified, the trainer will put the data on any available GPUs. loss_weights : A dictionary mapping loss names to numbers. Each loss will be multiplied by the corresponding value in the dictionary. If not specified, then no loss weighting will occur. label_mapper : A function that takes in a label and returns another label. For example, it might be useful to move a set of labels ranging from 100-200 to a range of 0-100, in which case you could pass in lambda x: x-100 . If not specified, then the original labels are used. sampler : The sampler used by the dataloader. If not specified, then random sampling will be used. collate_fn : The collate function used by the dataloader. record_keeper : An optional record_keeper object. record_keeper is a useful package for logging data during training and testing. You can use trainers without record_keeper , but if you'd like to use it, then pip install record_keeper and visit the record_keeper repo to learn more. lr_scheduers : A dictionary of PyTorch learning rate schedulers. Each scheduler will be stepped at the end of every epoch. gradient_clippers : A dictionary of gradient clipping functions. Each function will be called before the optimizers. freeze_trunk_batchnorm : If True, then the BatchNorm parameters of the trunk model will be frozen during training. label_hierarchy_level : If each sample in your dataset has multiple labels, then this integer argument can be used to select which \"level\" to use. This assumes that your labels are \"2-dimensional\" with shape (num_samples, num_hierarchy_levels). dataloader_num_workers : The number of processes your dataloader will use to load data. data_and_label_getter : A function that takes the output of your dataset's __getitem__ function, and returns a tuple of (data, labels). If None, then it is assumed that __getitem__ returns (data, labels).","title":"BaseTrainer"},{"location":"trainers/#metriclossonly","text":"This trainer just computes a metric loss from the output of your embedder network. trainers . MetricLossOnly ( ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"embedder\": embedder_model} loss_funcs : Must have the following form: {\"metric_loss\": loss_func}","title":"MetricLossOnly"},{"location":"trainers/#trainwithclassifier","text":"This trainer is for the case where your architecture is trunk -> embedder -> classifier. It applies a metric loss to the output of the embedder network, and a classification loss to the output of the classifier network. trainers . TrainWithClassifier ( ** kwargs ) Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"embedder\": embedder_model, \"classifier\": classifier_model} loss_funcs : Must have the following form: {\"metric_loss\": loss_func1, \"classifier_loss\": loss_func2}","title":"TrainWithClassifier"},{"location":"trainers/#cascadedembeddings","text":"This trainer is a generalization of Hard-Aware Deeply Cascaded Embedding . It splits the output of your embedder network, computing a separate loss for each section. In other words, the output of your embedder should be the concatenation of your cascaded models. trainers . CascadedEmbeddings ( embedding_sizes , ** kwargs ) Parameters : embedding_sizes: A list of integers, which represent the size of the output of each cascaded model. Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"embedder\": embedder_model} Optionally include key:values of the form \"classifier_%d\": classifier_model_%d. The integer appended to \"classifier_\" represents the cascaded model that the classifier will be appended to. For example, if the dictionary has classifier_0 and classifier_2, then the 0th and 2nd cascaded models will have classifier_model_0 and classifier_model_2 appended to them. loss_funcs : Must have the following form: {\"metric_loss_%d\": metric_loss_func_%d} Optionally include key:values of the form \"classifier_loss_%d\": classifier_loss_func_%d. The appended integer represents which cascaded model the loss applies to. mining_funcs : Must have the following form: {\"post_gradient_miner_%d\": mining_func_%d} *Optionally include \"pre_gradient_miner\": pre_gradient_miner","title":"CascadedEmbeddings"},{"location":"trainers/#deepadversarialmetriclearning","text":"This is an implementation of Deep Adversarial Metric Learning trainers . DeepAdversarialMetricLearning ( metric_alone_epochs = 0 , g_alone_epochs = 0 , g_triplets_per_anchor = 100 , ** kwargs ): Parameters : metric_alone_epochs : At the beginning of training, this many epochs will consist of only the metric_loss. g_alone_epochs : After metric_alone_epochs, this many epochs will consist of only the adversarial generator loss. g_triplets_per_anchor : The number of real triplets per sample that should be passed into the generator. For each real triplet, the generator will output a synthetic triplet. Requirements : models : Must have the following form: {\"trunk\": trunk_model, \"embedder\": embedder_model} Optionally include \"classifier\": classifier_model loss_funcs : Must have the following form: {\"metric_loss\": metric_loss, \"g_adv_loss\": g_adv_loss, \"synth_loss\": synth_loss} Optionally include \"classifier_loss\": classifier_loss metric_loss applies to the embeddings of real data. g_adv_loss is the adversarial generator loss. Currently, only TripletMarginLoss and AngularLoss are supported synth_loss applies to the embeddings of the synthetic generator triplets. Currently, only TripletMarginLoss and AngularLoss are supported loss_weights : Must be one of the following: None {\"metric_loss\": weight1, \"g_adv_loss\": weight2, \"synth_loss\": weight3, \"g_reg_loss\": weight4, \"g_hard_lss\": weight5} Optionally include \"classifier_loss\": classifier_loss \"g_reg_loss\" and \"g_hard_loss\" refer to the regularization losses described in the paper.","title":"DeepAdversarialMetricLearning"},{"location":"trainers/#unsupervisedembeddingsusingaugmentations","text":"This is an implementation of a general approach that has been used in recent unsupervised learning papers, e.g. Unsupervised Embedding Learning via Invariant and Spreading Instance Feature and Unsupervised Deep Metric Learning via Auxiliary Rotation Loss . The idea is that augmented versions of a datapoint should be close to each other in the embedding space. trainers . UnsupervisedEmbeddingsUsingAugmentations ( transforms , data_and_label_setter = None , ** kwargs ) Parameters : transforms : A list of transforms. For every sample in a batch, each transform will be applied to the sample. If there are N transforms and the batch size is B, then there will be a total of B*N augmented samples. data_and_label_setter : A function that takes in a tuple of (augmented_data, pseudo_labels) and outputs whatever is expected by self.data_and_label_getter.","title":"UnsupervisedEmbeddingsUsingAugmentations"},{"location":"utils/","text":"Docs coming soon","title":"Utils"}]}