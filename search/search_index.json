{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyTorch Metric Learning \u00b6 Installation \u00b6 pip install pytorch_metric_learning","title":"Home"},{"location":"#pytorch-metric-learning","text":"","title":"PyTorch Metric Learning"},{"location":"#installation","text":"pip install pytorch_metric_learning","title":"Installation"},{"location":"losses/","text":"Losses \u00b6 All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) Or if you are using a loss in conjunction with a miner: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output ) AngularLoss \u00b6 Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha , triplets_per_anchor = 100 , ** kwargs ) Parameters : alpha : The angle (as described in the paper), specified in degrees. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. ArcFaceLoss \u00b6 ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The angular margin penalty in degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) BaseMetricLossFunction \u00b6 All loss functions extend this class and therefore inherit its __init__ parameters. losses . BaseMetricLossFunction ( normalize_embeddings = True , num_class_per_param = None , learnable_param_names = None ) Parameters : normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before the loss is computed. num_class_per_param : If learnable_param_names is set, then this represents the number of classes for each parameter. If your parameters don't have a separate value for each class, then you can leave this at None. learnable_param_names : A list of strings where each element is the name of attributes that should be converted to nn.Parameter. If None, then no parameters are converted. Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError ContrastiveLoss \u00b6 losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , use_similarity = False , power = 1 , avg_non_zero_only = True , ** kwargs ): Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. use_similarity : If True, will use dot product between vectors instead of euclidean distance. power : Each pair's loss will be raised to this power. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss. FastAPLoss \u00b6 Deep Metric Learning to Rank losses . FastAPLoss ( num_bins , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision GenericPairLoss \u00b6 losses . GenericPairLoss ( use_similarity , iterate_through_loss , squared_distances = False , ** kwargs ) Parameters : use_similarity : Set to True if the loss function uses pairwise similarity (dot product of each embedding pair). Otherwise, euclidean distance will be used. iterate_through_loss : If True, then pairs are passed iteratively to self.pair_based_loss, by going through each sample in a batch, and selecting just the positive and negative pairs containing that sample. Otherwise, the pairs are passed to self.pair_based_loss all at once. squared_distances : If True, then the euclidean distance will be squared. Required Implementations : def pair_based_loss ( self , pos_pairs , neg_pairs , pos_pair_anchor_labels , neg_pair_anchor_labels ): raise NotImplementedError GeneralizedLiftedStructureLoss \u00b6 losses . GeneralizedLiftedStructureLoss ( neg_margin , ** kwargs ) Parameters : neg_margin : The margin in the expression e^(margin - negative_distance) MarginLoss \u00b6 Sampling Matters in Deep Embedding Learning losses . MarginLoss ( margin , nu , beta , triplets_per_anchor = 100 , ** kwargs ) Parameters : margin : The radius of the minimalbuffer between positive and negative pairs. nu : The regularization weight for the magnitude of beta. beta : The center of the minimal buffer between positive and negative pairs. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. To make beta a learnable parameter (as done in the paper), pass in the keyword argument: learnable_param_names = [ \"beta\" ] You can then pass the loss function's parameters() to any PyTorch optimizer. MultiSimilarityLoss \u00b6 Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning losses . MultiSimilarityLoss ( alpha , beta , base = 0.5 , ** kwargs ) Parameters : alpha : The weight applied to positive pairs. beta : The weight applied to negative pairs. base : The offset applied to the exponent in the loss. NCALoss \u00b6 Neighbourhood Components Analysis losses . NCALoss ( softmax_scale = 1 , ** kwargs ) Parameters : softmax_scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) NormalizedSoftmaxLoss \u00b6 Classification is a Strong Baseline for Deep Metric Learning losses . NormalizedSoftmaxLoss ( temperature , embedding_size , num_classes , ** kwargs ) Parameters : temperature : The exponent divisor in the softmax funtion. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. NPairsLoss \u00b6 Improved Deep Metric Learning with Multi-class N-pair Loss Objective losses . NPairsLoss ( l2_reg_weight = 0 , ** kwargs ) Parameters : l2_reg_weight : The regularization weight for the L2 norm of the embeddings. ProxyNCALoss \u00b6 No Fuss Distance Metric Learning using Proxies losses . ProxyNCALoss ( num_classes , embedding_size , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. softmax_scale : See NCALoss SignalToNoiseRatioContrastiveLoss \u00b6 Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning losses . SignalToNoiseRatioContrastiveLoss ( pos_margin , neg_margin , regularizer_weight , avg_non_zero_only = True , ** kwargs ) Parameters : pos_margin : The noise-to-signal ratio over which positive pairs will contribute to the loss. neg_margin : The noise-to-signal ratio under which negative pairs will contribute to the loss. regularizer_weight : The regularizer encourages the embeddings to have zero-mean distributions. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss. SoftTripleLoss \u00b6 SoftTriple Loss: Deep Metric Learning Without Triplet Sampling losses . SoftTripleLoss ( embedding_size , num_classes , centers_per_class , la = 20 , gamma = 0.1 , reg_weight = 0.2 , margin = 0.01 , ** kwargs ) Parameters : embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. centers_per_class : The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) la : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) gamma : The similarity-to-centers multiplier. reg_weight : The regularization weight which encourages class centers to be close to each other. margin : The margin in the expression e^(similarities - margin). TripletMarginLoss \u00b6 lossses . TripletMarginLoss ( margin = 0.05 , distance_norm = 2 , power = 1 , swap = False , smooth_loss = False , avg_non_zero_only = True , triplets_per_anchor = 100 , ** kwargs ) Parameters : margin : The desired difference between the anchor-positive distance and the anchor-negative distance. distance_norm : The norm used when calculating distance between embeddings power : Each pair's loss will be raised to this power. swap : Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more. smooth_loss : Use the log-exp version of the triplet loss avg_non_zero_only : Only triplets that contribute non-zero loss will be used in the final loss. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used.","title":"Losses"},{"location":"losses/#losses","text":"All loss functions are used as follows: from pytorch_metric_learning import losses loss_func = losses . SomeLoss () loss = loss_func ( embeddings , labels ) Or if you are using a loss in conjunction with a miner: from pytorch_metric_learning import miners , losses miner_func = miners . SomeMiner () loss_func = losses . SomeLoss () miner_output = miner_func ( embeddings , labels ) losses = loss_func ( embeddings , labels , miner_output )","title":"Losses"},{"location":"losses/#angularloss","text":"Deep Metric Learning with Angular Loss losses . AngularLoss ( alpha , triplets_per_anchor = 100 , ** kwargs ) Parameters : alpha : The angle (as described in the paper), specified in degrees. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used.","title":"AngularLoss"},{"location":"losses/#arcfaceloss","text":"ArcFace: Additive Angular Margin Loss for Deep Face Recognition losses . ArcFaceLoss ( margin , num_classes , embedding_size , scale = 64 , ** kwargs ) Parameters : margin : The angular margin penalty in degrees. num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)","title":"ArcFaceLoss"},{"location":"losses/#basemetriclossfunction","text":"All loss functions extend this class and therefore inherit its __init__ parameters. losses . BaseMetricLossFunction ( normalize_embeddings = True , num_class_per_param = None , learnable_param_names = None ) Parameters : normalize_embeddings : If True, embeddings will be normalized to have a Euclidean norm of 1 before the loss is computed. num_class_per_param : If learnable_param_names is set, then this represents the number of classes for each parameter. If your parameters don't have a separate value for each class, then you can leave this at None. learnable_param_names : A list of strings where each element is the name of attributes that should be converted to nn.Parameter. If None, then no parameters are converted. Required Implementations : def compute_loss ( self , embeddings , labels , indices_tuple = None ): raise NotImplementedError","title":"BaseMetricLossFunction"},{"location":"losses/#contrastiveloss","text":"losses . ContrastiveLoss ( pos_margin = 0 , neg_margin = 1 , use_similarity = False , power = 1 , avg_non_zero_only = True , ** kwargs ): Parameters : pos_margin : The distance (or similarity) over (under) which positive pairs will contribute to the loss. neg_margin : The distance (or similarity) under (over) which negative pairs will contribute to the loss. use_similarity : If True, will use dot product between vectors instead of euclidean distance. power : Each pair's loss will be raised to this power. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss.","title":"ContrastiveLoss"},{"location":"losses/#fastaploss","text":"Deep Metric Learning to Rank losses . FastAPLoss ( num_bins , ** kwargs ) Parameters : num_bins : The number of soft histogram bins for calculating average precision","title":"FastAPLoss"},{"location":"losses/#genericpairloss","text":"losses . GenericPairLoss ( use_similarity , iterate_through_loss , squared_distances = False , ** kwargs ) Parameters : use_similarity : Set to True if the loss function uses pairwise similarity (dot product of each embedding pair). Otherwise, euclidean distance will be used. iterate_through_loss : If True, then pairs are passed iteratively to self.pair_based_loss, by going through each sample in a batch, and selecting just the positive and negative pairs containing that sample. Otherwise, the pairs are passed to self.pair_based_loss all at once. squared_distances : If True, then the euclidean distance will be squared. Required Implementations : def pair_based_loss ( self , pos_pairs , neg_pairs , pos_pair_anchor_labels , neg_pair_anchor_labels ): raise NotImplementedError","title":"GenericPairLoss"},{"location":"losses/#generalizedliftedstructureloss","text":"losses . GeneralizedLiftedStructureLoss ( neg_margin , ** kwargs ) Parameters : neg_margin : The margin in the expression e^(margin - negative_distance)","title":"GeneralizedLiftedStructureLoss"},{"location":"losses/#marginloss","text":"Sampling Matters in Deep Embedding Learning losses . MarginLoss ( margin , nu , beta , triplets_per_anchor = 100 , ** kwargs ) Parameters : margin : The radius of the minimalbuffer between positive and negative pairs. nu : The regularization weight for the magnitude of beta. beta : The center of the minimal buffer between positive and negative pairs. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used. To make beta a learnable parameter (as done in the paper), pass in the keyword argument: learnable_param_names = [ \"beta\" ] You can then pass the loss function's parameters() to any PyTorch optimizer.","title":"MarginLoss"},{"location":"losses/#multisimilarityloss","text":"Multi-Similarity Loss with General Pair Weighting for Deep Metric Learning losses . MultiSimilarityLoss ( alpha , beta , base = 0.5 , ** kwargs ) Parameters : alpha : The weight applied to positive pairs. beta : The weight applied to negative pairs. base : The offset applied to the exponent in the loss.","title":"MultiSimilarityLoss"},{"location":"losses/#ncaloss","text":"Neighbourhood Components Analysis losses . NCALoss ( softmax_scale = 1 , ** kwargs ) Parameters : softmax_scale : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.)","title":"NCALoss"},{"location":"losses/#normalizedsoftmaxloss","text":"Classification is a Strong Baseline for Deep Metric Learning losses . NormalizedSoftmaxLoss ( temperature , embedding_size , num_classes , ** kwargs ) Parameters : temperature : The exponent divisor in the softmax funtion. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset.","title":"NormalizedSoftmaxLoss"},{"location":"losses/#npairsloss","text":"Improved Deep Metric Learning with Multi-class N-pair Loss Objective losses . NPairsLoss ( l2_reg_weight = 0 , ** kwargs ) Parameters : l2_reg_weight : The regularization weight for the L2 norm of the embeddings.","title":"NPairsLoss"},{"location":"losses/#proxyncaloss","text":"No Fuss Distance Metric Learning using Proxies losses . ProxyNCALoss ( num_classes , embedding_size , ** kwargs ) Parameters : num_classes : The number of classes in your training dataset. embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. softmax_scale : See NCALoss","title":"ProxyNCALoss"},{"location":"losses/#signaltonoiseratiocontrastiveloss","text":"Signal-to-Noise Ratio: A Robust Distance Metric for Deep Metric Learning losses . SignalToNoiseRatioContrastiveLoss ( pos_margin , neg_margin , regularizer_weight , avg_non_zero_only = True , ** kwargs ) Parameters : pos_margin : The noise-to-signal ratio over which positive pairs will contribute to the loss. neg_margin : The noise-to-signal ratio under which negative pairs will contribute to the loss. regularizer_weight : The regularizer encourages the embeddings to have zero-mean distributions. avg_non_zero_only : Only pairs that contribute non-zero loss will be used in the final loss.","title":"SignalToNoiseRatioContrastiveLoss"},{"location":"losses/#softtripleloss","text":"SoftTriple Loss: Deep Metric Learning Without Triplet Sampling losses . SoftTripleLoss ( embedding_size , num_classes , centers_per_class , la = 20 , gamma = 0.1 , reg_weight = 0.2 , margin = 0.01 , ** kwargs ) Parameters : embedding_size : The size of the embeddings that you pass into the loss function. For example, if your batch size is 128 and your network outputs 512 dimensional embeddings, then set embedding_size to 512. num_classes : The number of classes in your training dataset. centers_per_class : The number of weight vectors per class. (The regular cross entropy loss has 1 center per class.) la : The exponent multiplier in the loss's softmax expression. (This is the inverse of the softmax temperature.) gamma : The similarity-to-centers multiplier. reg_weight : The regularization weight which encourages class centers to be close to each other. margin : The margin in the expression e^(similarities - margin).","title":"SoftTripleLoss"},{"location":"losses/#tripletmarginloss","text":"lossses . TripletMarginLoss ( margin = 0.05 , distance_norm = 2 , power = 1 , swap = False , smooth_loss = False , avg_non_zero_only = True , triplets_per_anchor = 100 , ** kwargs ) Parameters : margin : The desired difference between the anchor-positive distance and the anchor-negative distance. distance_norm : The norm used when calculating distance between embeddings power : Each pair's loss will be raised to this power. swap : Use the positive-negative distance instead of anchor-negative distance, if it violates the margin more. smooth_loss : Use the log-exp version of the triplet loss avg_non_zero_only : Only triplets that contribute non-zero loss will be used in the final loss. triplets_per_anchor : The number of triplets per element to sample within a batch. Can be an integer or the string \"all\". For example, if your batch size is 128, and triplets_per_anchor is 100, then 12800 triplets will be sampled. If triplets_per_anchor is \"all\", then all possible triplets in the batch will be used.","title":"TripletMarginLoss"},{"location":"miners/","text":"Docs coming soon","title":"Miners"}]}